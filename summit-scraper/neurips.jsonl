{'custom_id': '2465f660-f693-43ee-b507-fa7e41b7ac3b', 'prompt': {'id': 'pmpt_6865fea7a73481979d2a20d7492a8b990e375d240959c724', 'version': None, 'variables': {'title': 'Bench2Drive: Towards Multi-Ability Benchmarking of Closed-Loop End-To-End Autonomous Driving', 'abstract': "In an era marked by the rapid scaling of foundation models, autonomous driving technologies are approaching a transformative threshold where end-to-end autonomous driving (E2E-AD) emerges due to its potential of scaling up in the data-driven manner. However, existing E2E-AD methods are mostly evaluated under the open-loop log-replay manner with L2 errors and collision rate as metrics (e.g., in nuScenes), which could not fully reflect the driving performance of algorithms as recently acknowledged in the community. For those E2E-AD methods evaluated under the closed-loop protocol, they are tested in fixed routes (e.g., Town05Long and Longest6 in CARLA) with the driving score as metrics, which is known for high variance due to the unsmoothed metric function and large randomness in the long route. Besides, these methods usually collect their own data for training, which makes algorithm-level fair comparison infeasible.    To fulfill the paramount need of comprehensive, realistic, and fair testing environments for Full Self-Driving (FSD), we present Bench2Drive, the first benchmark for evaluating E2E-AD systems' multiple abilities in a closed-loop manner. Bench2Drive's official training data consists of 2 million fully annotated frames, collected from 10000 short clips uniformly distributed under 44 interactive scenarios (cut-in, overtaking, detour, etc), 23 weathers (sunny, foggy, rainy, etc), and 12 towns (urban, village, university, etc) in CARLA v2. Its evaluation protocol requires E2E-AD models to pass 44 interactive scenarios under different locations and weathers which sums up to 220 routes and thus provides a comprehensive and disentangled assessment about their driving capability under different situations. We implement state-of-the-art E2E-AD models and evaluate them in Bench2Drive, providing insights regarding current status and future directions.", 'venue': 'NeurIPS', 'published_date': '2024-12-01'}}}
{'custom_id': '3fa947c9-e0f4-4aac-a2c3-bafd7ed59436', 'prompt': {'id': 'pmpt_6865fea7a73481979d2a20d7492a8b990e375d240959c724', 'version': None, 'variables': {'title': 'Deep Policy Gradient Methods Without Batch Updates, Target Networks, or Replay Buffers', 'abstract': 'Modern deep policy gradient methods achieve effective performance on simulated robotic tasks, but they all require large replay buffers or expensive batch updates, or both, making them incompatible for real systems with resource-limited computers. We show that these methods fail catastrophically when limited to small replay buffers or during incremental learning, where updates only use the most recent sample without batch updates or a replay buffer. We propose a novel incremental deep policy gradient method --- Action Value Gradient (AVG) and a set of normalization and scaling techniques to address the challenges of instability in incremental learning. On robotic simulation benchmarks, we show that AVG is the only incremental method that learns effectively, often achieving final performance comparable to batch policy gradient methods. This advancement enabled us to show for the first time effective deep reinforcement learning with real robots using only incremental updates, employing a robotic manipulator and a mobile robot.', 'venue': 'NeurIPS', 'published_date': '2024-12-01'}}}
{'custom_id': '4a4de4f9-bc6b-4b15-b132-07a2eb62088b', 'prompt': {'id': 'pmpt_6865fea7a73481979d2a20d7492a8b990e375d240959c724', 'version': None, 'variables': {'title': 'Towards Understanding How Transformers Learn In-context Through a Representation Learning Lens', 'abstract': "Pre-trained large language models based on Transformers have demonstrated remarkable in-context learning (ICL) abilities. With just a few demonstration examples, the models can implement new tasks without any parameter updates. However, it is still an open question to understand the mechanism of ICL. In this paper, we attempt to explore the ICL process in Transformers through a lens of representation learning. Initially, leveraging kernel methods, we figure out a dual model for one softmax attention layer. The ICL inference process of the attention layer aligns with the training procedure of its  dual model, generating token representation predictions that are equivalent to the dual model's test outputs. We delve into the training process of this  dual model from a representation learning standpoint and further derive a generalization error bound related to the quantity of demonstration tokens. Subsequently, we extend our theoretical conclusions to more complicated scenarios, including one Transformer layer and multiple attention layers. Furthermore, drawing inspiration from existing representation learning methods especially contrastive learning, we propose potential modifications for the attention layer. Finally, experiments are designed to support our findings.", 'venue': 'NeurIPS', 'published_date': '2024-12-01'}}}
{'custom_id': '8b575eb5-516e-41d3-9bb9-4c652b164464', 'prompt': {'id': 'pmpt_6865fea7a73481979d2a20d7492a8b990e375d240959c724', 'version': None, 'variables': {'title': 'FINALLY: fast and universal speech enhancement with studio-like quality', 'abstract': "In this paper, we address the challenge of speech enhancement in real-world recordings, which often contain various forms of distortion, such as background noise, reverberation, and microphone artifacts.We revisit the use of Generative Adversarial Networks (GANs) for speech enhancement and theoretically show that GANs are naturally inclined to seek the point of maximum density within the conditional clean speech distribution, which, as we argue, is essential for speech enhancement task.We study various feature extractors for perceptual loss to facilitate the stability of adversarial training, developing a methodology for probing the structure of the feature space.This leads us to integrate WavLM-based perceptual loss into MS-STFT adversarial training pipeline, creating an effective and stable training procedure for the speech enhancement model.The resulting speech enhancement model, which we refer to as FINALLY, builds upon the HiFi++ architecture, augmented with a WavLM encoder and a novel training pipeline.Empirical results on various datasets confirm our model's ability to produce clear, high-quality speech at 48 kHz, achieving state-of-the-art performance in the field of speech enhancement. Demo page: https://samsunglabs.github.io/FINALLY-page/", 'venue': 'NeurIPS', 'published_date': '2024-12-01'}}}
{'custom_id': '8d1bae45-e495-4db9-9b46-6ceaf09d9dbb', 'prompt': {'id': 'pmpt_6865fea7a73481979d2a20d7492a8b990e375d240959c724', 'version': None, 'variables': {'title': 'Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate', 'abstract': 'Generalization remains a central challenge in machine learning. In this work, we propose Learning from Teaching (LoT), a novel regularization technique for deep neural networks to enhance generalization. Inspired by the human ability to capture concise and abstract patterns, we hypothesize that generalizable correlations are expected to be easier to imitate. LoT operationalizes this concept to improve the generalization of the main model with auxiliary student learners. The student learners are trained by the main model and, in turn, provide feedback to help the main model capture more generalizable and imitable correlations. Our experimental results across several domains, including Computer Vision, Natural Language Processing, and methodologies like Reinforcement Learning, demonstrate that the introduction of LoT brings significant benefits compared to training models on the original dataset. The results suggest the effectiveness and efficiency of LoT in identifying generalizable information at the right scales while discarding spurious data correlations, thus making LoT a valuable addition to current machine learning. Code is available at https://github.com/jincan333/LoT.', 'venue': 'NeurIPS', 'published_date': '2024-12-01'}}}
{'custom_id': '3f314e75-4275-4299-9a9b-a98a0d66e8d5', 'prompt': {'id': 'pmpt_6865fea7a73481979d2a20d7492a8b990e375d240959c724', 'version': None, 'variables': {'title': 'Prospective Representation Learning for Non-Exemplar Class-Incremental Learning', 'abstract': 'Non-exemplar class-incremental learning (NECIL) is a challenging task that requires recognizing both old and new classes without retaining any old class samples. Current works mainly deal with the conflicts between old and new classes retrospectively as a new task comes in. However, the lack of old task data makes balancing old and new classes difficult. Instead, we propose a Prospective Representation Learning (PRL) approach to prepare the model for handling conflicts in advance. In the base phase, we squeeze the embedding distribution of the current classes to reserve space for forward compatibility with future classes. In the incremental phase, we make the new class features away from the saved prototypes of old classes in a latent space while aligning the current embedding space with the latent space when updating the model. Thereby, the new class features are clustered in the reserved space to minimize the shock of the new classes on the former classes. Our approach can help existing NECIL baselines to balance old and new classes in a plug-and-play manner. Extensive experiments on several benchmarks demonstrate that our approach outperforms the state-of-the-art methods.', 'venue': 'NeurIPS', 'published_date': '2024-12-01'}}}
{'custom_id': 'cc8abf75-95fd-49df-93a3-e536ae0eedac', 'prompt': {'id': 'pmpt_6865fea7a73481979d2a20d7492a8b990e375d240959c724', 'version': None, 'variables': {'title': 'Boosting Sample Efficiency and Generalization in Multi-agent Reinforcement Learning via Equivariance', 'abstract': 'Multi-Agent Reinforcement Learning (MARL) struggles with sample inefficiency and poor generalization [1]. These challenges are partially due to a lack of structure or inductive bias in the neural networks typically used in learning the policy. One such form of structure that is commonly observed in multi-agent scenarios is symmetry. The field of Geometric Deep Learning has developed Equivariant Graph Neural Networks (EGNN) that are equivariant (or symmetric) to rotations, translations, and reflections of nodes. Incorporating equivariance has been shown to improve learning efficiency and decrease error [ 2 ]. In this paper, we demonstrate that EGNNs improve the sample efficiency and generalization in MARL. However, we also show that a naive application of EGNNs to MARL results in poor early exploration due to a bias in the EGNN structure. To mitigate this bias, we present Exploration-enhanced Equivariant Graph Neural Networks or E2GN2. We compare E2GN2 to other common function approximators using common MARL benchmarks MPE and SMACv2. E2GN2 demonstrates a significant improvement in sample efficiency, greater final reward convergence, and a 2x-5x gain in over standard GNNs in our generalization tests. These results pave the way for more reliable and effective solutions in complex multi-agent systems.', 'venue': 'NeurIPS', 'published_date': '2024-12-01'}}}
{'custom_id': '23a01ffc-3c28-4225-9a7a-8938ebace420', 'prompt': {'id': 'pmpt_6865fea7a73481979d2a20d7492a8b990e375d240959c724', 'version': None, 'variables': {'title': 'TurboHopp: Accelerated Molecule Scaffold Hopping with Consistency Models', 'abstract': 'Navigating the vast chemical space of druggable compounds is a formidable challenge in drug discovery, where generative models are increasingly employed to identify viable candidates. Conditional 3D structure-based drug design (3D-SBDD) models, which take into account complex three-dimensional interactions and molecular geometries, are particularly promising. Scaffold hopping is an efficient strategy that facilitates the identification of similar active compounds by strategically modifying the core structure of molecules, effectively narrowing the wide chemical space and enhancing the discovery of drug-like products. However, the practical application of 3D-SBDD generative models is hampered by their slow processing speeds. To address this bottleneck, we introduce TurboHopp, an accelerated pocket-conditioned 3D scaffold hopping model that merges the strategic effectiveness of traditional scaffold hopping with rapid generation capabilities of consistency models. This synergy not only enhances efficiency but also significantly boosts generation speeds, achieving up to 30 times faster inference speed as well as superior generation quality compared to existing diffusion-based models, establishing TurboHopp as a powerful tool in drug discovery. Supported by faster inference speed, we further optimize our model, using Reinforcement Learning for Consistency Models (RLCM), to output desirable molecules. We demonstrate the broad applicability of TurboHopp across multiple drug discovery scenarios, underscoring its potential in diverse molecular settings.The code is provided at https://github.com/orgw/TurboHopp', 'venue': 'NeurIPS', 'published_date': '2024-12-01'}}}
{'custom_id': 'e4a0b6e4-eb0e-49a0-89c1-96d5ac61f1a6', 'prompt': {'id': 'pmpt_6865fea7a73481979d2a20d7492a8b990e375d240959c724', 'version': None, 'variables': {'title': 'WikiDBs: A Large-Scale Corpus Of Relational Databases From Wikidata', 'abstract': 'Deep learning on tabular data, and particularly tabular representation learning, has recently gained growing interest. However, representation learning for relational databases with multiple tables is still an underexplored area, which may be attributed to the lack of openly available resources. To support the development of foundation models for tabular data and relational databases, we introduce WikiDBs, a novel open-source corpus of 100,000 relational databases. Each database consists of multiple tables connected by foreign keys. The corpus is based on Wikidata and aims to follow certain characteristics of real-world databases. In this paper, we describe the dataset and our method for creating it. By making our code publicly available, we enable others to create tailored versions of the dataset, for example, by creating databases in different languages. Finally, we conduct a set of initial experiments to showcase how WikiDBs can be used to train for data engineering tasks, such as missing value imputation and column type annotation.', 'venue': 'NeurIPS', 'published_date': '2024-12-01'}}}
{'custom_id': '77477aa6-af7c-4ebe-a0e7-0ebaeaa4ef34', 'prompt': {'id': 'pmpt_6865fea7a73481979d2a20d7492a8b990e375d240959c724', 'version': None, 'variables': {'title': 'Mixtures of Experts for Audio-Visual Learning', 'abstract': 'With the rapid development of multimedia technology, audio-visual learning has emerged as a promising research topic within the field of multimodal analysis. In this paper, we explore parameter-efficient transfer learning for audio-visual learning and propose the Audio-Visual Mixture of Experts (\\ourmethodname) to inject adapters into pre-trained models flexibly. Specifically, we introduce unimodal and cross-modal adapters as multiple experts to specialize in intra-modal and inter-modal information, respectively, and employ a lightweight router to dynamically allocate the weights of each expert according to the specific demands of each task. Extensive experiments demonstrate that our proposed approach \\ourmethodname achieves superior performance across multiple audio-visual tasks, including AVE, AVVP, AVS, and AVQA. Furthermore, visual-only experimental results also indicate that our approach can tackle challenging scenes where modality information is missing.The source code is available at \\url{https://github.com/yingchengy/AVMOE}.', 'venue': 'NeurIPS', 'published_date': '2024-12-01'}}}
{'custom_id': 'cfaaf598-1106-4a5a-aaf1-a056d0daa903', 'prompt': {'id': 'pmpt_6865fea7a73481979d2a20d7492a8b990e375d240959c724', 'version': None, 'variables': {'title': 'The Expressive Capacity of State Space Models: A Formal Language Perspective', 'abstract': 'Recently, recurrent models based on linear state space models  (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba.', 'venue': 'NeurIPS', 'published_date': '2024-12-01'}}}
{'custom_id': 'cca9fa4d-f850-41e6-aa3c-06f7391c7ca2', 'prompt': {'id': 'pmpt_6865fea7a73481979d2a20d7492a8b990e375d240959c724', 'version': None, 'variables': {'title': 'NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking', 'abstract': "Benchmarking vision-based driving policies is challenging. On one hand, open-loop evaluation with real data is easy, but these results do not reflect closed-loop performance. On the other, closed-loop evaluation is possible in simulation, but is hard to scale due to its significant computational demands. Further, the simulators available today exhibit a large domain gap to real data. This has resulted in an inability to draw clear conclusions from the rapidly growing body of research on end-to-end autonomous driving. In this paper, we present NAVSIM, a middle ground between these evaluation paradigms, where we use large datasets in combination with a non-reactive simulator to enable large-scale real-world benchmarking. Specifically, we gather simulation-based metrics, such as progress and time to collision, by unrolling bird's eye view abstractions of the test scenes for a short simulation horizon. Our simulation is non-reactive, i.e., the evaluated policy and environment do not influence each other. As we demonstrate empirically, this decoupling allows open-loop metric computation while being better aligned with closed-loop evaluations than traditional displacement errors. NAVSIM enabled a new competition held at CVPR 2024, where 143 teams submitted 463 entries, resulting in several new insights. On a large set of challenging scenarios, we observe that simple methods with moderate compute requirements such as TransFuser can match recent large-scale end-to-end driving architectures such as UniAD. Our modular framework can potentially be extended with new datasets, data curation strategies, and metrics, and will be continually maintained to host future challenges. Our code is available at https://github.com/autonomousvision/navsim.", 'venue': 'NeurIPS', 'published_date': '2024-12-01'}}}
{'custom_id': '3f662ac5-b9c7-4317-99ac-8320fe3f71f0', 'prompt': {'id': 'pmpt_6865fea7a73481979d2a20d7492a8b990e375d240959c724', 'version': None, 'variables': {'title': 'Is One GPU Enough? Pushing Image Generation at Higher-Resolutions with Foundation Models.', 'abstract': "In this work, we introduce Pixelsmith, a zero-shot text-to-image generative framework to sample images at higher resolutions with a single GPU. We are the first to show that it is possible to scale the output of a pre-trained diffusion model by a factor of 1000, opening the road to gigapixel image generation at no extra cost. Our cascading method uses the image generated at the lowest resolution as baseline to sample at higher resolutions. For the guidance, we introduce the Slider, a mechanism that fuses the overall structure contained in the first-generated image with enhanced fine details. At each inference step, we denoise patches rather than the entire latent space, minimizing memory demands so that a single GPU can handle the process, regardless of the image's resolution. Our experimental results show that this method not only achieves higher quality and diversity compared to existing techniques but also reduces sampling time and ablation artifacts.", 'venue': 'NeurIPS', 'published_date': '2024-12-01'}}}
{'custom_id': 'b43f2cd6-d46c-429b-a21f-ace769defe8d', 'prompt': {'id': 'pmpt_6865fea7a73481979d2a20d7492a8b990e375d240959c724', 'version': None, 'variables': {'title': 'Optimal Algorithms for Online Convex Optimization with Adversarial Constraints', 'abstract': 'A well-studied generalization of the standard online convex optimization (OCO) framework is constrained online convex optimization (COCO). In COCO, on every round, a convex cost function and a convex constraint function are revealed to the learner after it chooses the action for that round. The objective is to design an online learning policy that simultaneously achieves a small regret while ensuring a small cumulative constraint violation (CCV) against an adaptive adversary interacting over a horizon of length $T$. A long-standing open question in COCO is whether an online policy can simultaneously achieve $O(\\sqrt{T})$ regret and $\\tilde{O}(\\sqrt{T})$ CCV without any restrictive assumptions. For the first time, we answer this in the affirmative and show that a simple first-order policy can simultaneously achieve these bounds. Furthermore, in the case of strongly convex cost and convex constraint functions, the regret guarantee can be improved to $O(\\log T)$ while keeping the CCV bound the same as above. We establish these results by effectively combining adaptive OCO policies as a blackbox with Lyapunov optimization - a classic tool from control theory. Surprisingly, the analysis is short and elegant.', 'venue': 'NeurIPS', 'published_date': '2024-12-01'}}}
{'custom_id': '1cb4404b-bd50-446b-bc0c-952a9b0aa07d', 'prompt': {'id': 'pmpt_6865fea7a73481979d2a20d7492a8b990e375d240959c724', 'version': None, 'variables': {'title': 'Unveiling the Bias Impact on Symmetric Moral Consistency of Large Language Models', 'abstract': "Large Language Models (LLMs) have demonstrated remarkable capabilities, surpassing human experts in various benchmark tests and playing a vital role in various industry sectors. Despite their effectiveness, a notable drawback of LLMs is their inconsistent moral behavior, which raises ethical concerns. This work delves into symmetric moral consistency in large language models and demonstrates that modern LLMs lack sufficient consistency ability in moral scenarios. Our extensive investigation of twelve popular LLMs reveals that their assessed consistency scores are influenced by position bias and selection bias rather than their intrinsic abilities. We propose a new framework tSMC, which gauges the effects of these biases and effectively mitigates the bias impact based on the Kullback–Leibler divergence to pinpoint LLMs' mitigated Symmetric Moral Consistency. We find that the ability of LLMs to maintain consistency varies across different moral scenarios. Specifically, LLMs show more consistency in scenarios with clear moral answers compared to those where no choice is morally perfect. The average consistency score of 12 LLMs ranges from $60.7\\%$ in high-ambiguity moral scenarios to $84.8\\%$ in low-ambiguity moral scenarios.", 'venue': 'NeurIPS', 'published_date': '2024-12-01'}}}
{'custom_id': '0e0fa823-9614-4e48-a391-945d193c1537', 'prompt': {'id': 'pmpt_6865fea7a73481979d2a20d7492a8b990e375d240959c724', 'version': None, 'variables': {'title': 'Accelerated Regularized Learning in Finite N-Person Games', 'abstract': "Motivated by the success of Nesterov's accelerated gradient algorithm for convex minimization problems, we examine whether it is possible to achieve similar performance gains in the context of online learning in games.To that end, we introduce a family of accelerated learning methods, which we call “follow the accelerated leader” (FTXL), and which incorporates the use of momentum within the general framework of regularized learning - and, in particular, the exponential / multiplicative weights algorithm and its variants.Drawing inspiration and techniques from the continuous-time analysis of Nesterov's algorithm, we show that FTXL converges locally to strict Nash equilibria at a superlinear rate, achieving in this way an exponential speed-up over vanilla regularized learning methods (which, by comparison, converge to strict equilibria at a geometric, linear rate).Importantly, the FTXL maintains its superlinear convergence rate in a broad range of feedback structures, from deterministic, full information models to stochastic, realization-based ones, and even bandit, payoff-based information, where players are only able to observe their individual realized payoffs.", 'venue': 'NeurIPS', 'published_date': '2024-12-01'}}}
{'custom_id': '55cb9fe8-f875-42e0-a3f4-816fe3f0a277', 'prompt': {'id': 'pmpt_6865fea7a73481979d2a20d7492a8b990e375d240959c724', 'version': None, 'variables': {'title': 'Embedding Dimension of Contrastive Learning and $k$-Nearest Neighbors', 'abstract': 'We study the embedding dimension of distance comparison data in two settings: contrastive learning and $k$-nearest neighbors ($k$-NN). In both cases, the goal is to find the smallest dimension $d$ of an $\\ell_p$-space in which a given dataset can be represented. We show that the arboricity of the associated graphs plays a key role in designing embeddings. Using this approach, for the most frequently used $\\ell_2$-distance, we get matching upper and lower bounds in both settings.    In contrastive learning, we are given $m$ labeled samples of the form $(x_i, y_i^+, z_i^-)$ representing the fact that the positive example $y_i$ is closer to the anchor $x_i$ than the negative example $z_i$. We show that for representing such dataset in:- $\\ell_2$: $d = \\Theta(\\sqrt{m})$ is necessary and sufficient.- $\\ell_p$ for $p \\ge 1$: $d = O(m)$ is sufficient and $d = \\tilde \\Omega(\\sqrt{m})$ is necessary.- $\\ell_\\infty$: $d = O(m^{2/3})$ is sufficient and $d = \\tilde \\Omega(\\sqrt{m})$ is necessary.We also give results for the more general scenario when $t$ negatives are allowed.In $k$-NN, for each of the $n$ data points we are given an ordered set of the closest $k$ points. We show that for preserving the ordering of the $k$-NN for every point in:- $\\ell_2$: $d = \\Theta(k)$ is necessary and sufficient.- $\\ell_p$ for $p \\ge 1$: $d = \\tilde O(k^2)$ is sufficient and $d=\\tilde \\Omega(k)$ is necessary.- $\\ell_\\infty$ : $d = \\tilde \\Omega(k)$ is necessary.Furthermore, if the goal is to not just preserve the ordering of the $k$-NN but also keep them as the nearest neighbors then $d = \\tilde O (\\mathrm{poly}(k))$ suffices in $\\ell_p$ for $p \\ge 1$.', 'venue': 'NeurIPS', 'published_date': '2024-12-01'}}}
{'custom_id': '8a1b18e3-d7e7-4045-b388-5225138b02d9', 'prompt': {'id': 'pmpt_6865fea7a73481979d2a20d7492a8b990e375d240959c724', 'version': None, 'variables': {'title': 'Verified Code Transpilation with LLMs', 'abstract': "Domain-specific languages (DSLs) have become integral to various software workflows. Such languages offer domain-specific optimizations and abstractions that improve code readability and maintainability.  However, leveraging these languages requires developers to rewrite existing code using the specific DSL's API. While large language models (LLMs) have shown some success in automatic code transpilation, none of them provide any functional correctness guarantees on the rewritten code. Another approach for automating this task is verified lifting, which relies on program synthesis to find programs in the target language that are functionally equivalent to the source language program. While several verified lifting tools have been developed for various application domains, they are specialized for specific source-target languages or require significant expertise in domain knowledge to make the search efficient. In this paper, leveraging recent advances in LLMs, we propose an LLM-based approach (LLMLift) to building verified lifting tools. We use the LLM's capabilities to reason about programs to translate a given program into its corresponding equivalent in the target language. Additionally, we use LLMs to generate proofs for functional equivalence. We develop lifting-based compilers for four DSLs targeting different application domains. Our approach not only outperforms previous symbolic-based tools in number of benchmarks transpiled and transpilation time, but also requires significantly less effort to build.", 'venue': 'NeurIPS', 'published_date': '2024-12-01'}}}
{'custom_id': '6684deb3-f1e4-4810-b824-750477c5acf2', 'prompt': {'id': 'pmpt_6865fea7a73481979d2a20d7492a8b990e375d240959c724', 'version': None, 'variables': {'title': 'SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors', 'abstract': 'Popular parameter-efficient fine-tuning (PEFT) methods, such as LoRA and its variants, freeze pre-trained model weights $\\(\\mathbf{W}\\)$ and inject learnable matrices $\\(\\mathbf{\\Delta W}\\)$. These $\\(\\mathbf{\\Delta W}\\)$ matrices are structured for efficient parameterization, often using techniques like low-rank approximations or scaling vectors. However, these methods typically exhibit a performance gap compared to full fine-tuning. While recent PEFT methods have narrowed this gap, they do so at the expense of additional learnable parameters. We propose SVFT, a *simple* approach that structures $\\(\\mathbf{\\Delta W}\\)$ based on the specific weight matrix $\\(\\mathbf{W}\\)$. SVFT updates $\\(\\mathbf{W}\\)$ as a sparse combination $\\(M\\)$ of outer products of its singular vectors, training only the coefficients of these combinations. Crucially, we make additional off-diagonal elements in $M$ learnable, enabling a smooth trade-off between trainable parameters and expressivity—an aspect that distinctly sets our approach apart from previous works leveraging singular values. Extensive experiments on language and vision benchmarks show that SVFT recovers up to **96%** of full fine-tuning performance while training only **0.006 to 0.25%** of parameters, outperforming existing methods that achieve only up to **{85\\%}** performance with **0.03 to 0.8%** of the trainable parameter budget.', 'venue': 'NeurIPS', 'published_date': '2024-12-01'}}}
{'custom_id': '799de2f7-112a-45b7-9046-8e8d7168e798', 'prompt': {'id': 'pmpt_6865fea7a73481979d2a20d7492a8b990e375d240959c724', 'version': None, 'variables': {'title': 'Interpretable Image Classification with Adaptive Prototype-based Vision Transformers', 'abstract': "We present ProtoViT, a method for interpretable image classification combining deep learning and case-based reasoning. This method classifies an image by comparing it to a set of learned prototypes, providing explanations of the form ``this looks like that.'' In our model, a prototype consists of parts, which can deform over irregular geometries to create a better comparison between images. Unlike existing models that rely on Convolutional Neural Network (CNN) backbones and spatially rigid prototypes, our model integrates Vision Transformer (ViT) backbones into prototype based models, while offering spatially deformed prototypes that not only accommodate geometric variations of objects but also provide coherent and clear prototypical feature representations with an adaptive number of prototypical parts. Our experiments show that our model can generally achieve higher performance than the existing prototype based models. Our comprehensive analyses ensure that the prototypes are consistent and the interpretations are faithful.", 'venue': 'NeurIPS', 'published_date': '2024-12-01'}}}
